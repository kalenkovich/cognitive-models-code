{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_numbers = {\n",
    "    'A': [0, 1, 2, 3, 4, 6, 8],\n",
    "    'B': [2, 3, 4, 5, 7, 8, 9],\n",
    "    'C': [0, 1, 2, 5],\n",
    "    'D': [2, 3, 4, 5, 7, 9],\n",
    "    'E': [0, 1, 2, 5, 6],\n",
    "    'F': [0, 1, 2, 6],\n",
    "    'G': [0, 1, 2, 4, 5, 8],\n",
    "    'H': [0, 1, 3, 4, 6, 8],\n",
    "    'I': [2, 5, 7, 9],\n",
    "    'J': [0, 3, 4, 5],\n",
    "    'K': [0, 1, 6, 11, 12],\n",
    "    'L': [0, 1, 5],\n",
    "    'M': [0, 1, 3, 4, 10, 11],\n",
    "    'N': [0, 1, 3, 4, 10, 12],\n",
    "    'O': [0, 1, 2, 3, 4, 5],\n",
    "    'P': [0, 1, 2, 3, 6, 8],\n",
    "    'Q': [0, 1, 2, 3, 4, 5, 12],\n",
    "    'R': [0, 1, 2, 3, 6, 8, 12],\n",
    "    'S': [1, 2, 4, 5, 6, 8],\n",
    "    'T': [2, 7, 9],\n",
    "    'U': [0, 1, 3, 4, 5],\n",
    "    'V': [0, 1, 11, 13],\n",
    "    'W': [0, 1, 3, 4, 12, 13],\n",
    "    'X': [10, 11, 12, 13],\n",
    "    'Y': [9, 10, 11],\n",
    "    'Z': [2, 5, 11, 13]\n",
    "}\n",
    "\n",
    "feature_count = 14\n",
    "features_binary = {\n",
    "    letter: [1 if i in feature_list else 0 for i in range(feature_count)]\n",
    "    for letter, feature_list in feature_numbers.items()}\n",
    "letter_count = len(list(feature_numbers.keys()))\n",
    "alphabet = sorted(feature_numbers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAM(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Parameters\n",
    "        self.M = 1.0  # maximum activation\n",
    "        self.m = -0.2  # minimum activation\n",
    "        self.theta = 0.07  # decay rate\n",
    "        self.r_feature = 0  # baseline activation of the feature nodes\n",
    "        self.r_letter = 0  # baseline activation of the letter nodes\n",
    "        self.position_count = 4  # number of letters\n",
    "        self.letter_to_feature_excitatory = 0.005\n",
    "        self.letter_to_feature_inhibitory = 0.15\n",
    "        \n",
    "        # Nodes\n",
    "        self.initialize_nodes()\n",
    "        \n",
    "        # Weights\n",
    "        self.letter_to_letter_weights = np.zeros((self.letter_nodes.size, \n",
    "                                                  self.letter_nodes.size))\n",
    "        is_excitatory = np.array([features_binary[letter] \n",
    "                                  for letter \n",
    "                                  in sorted(features_binary.keys())]).T\n",
    "        # For one position\n",
    "        feature_to_letter_weights_1 = np.where(\n",
    "            is_excitatory,\n",
    "            self.letter_to_feature_excitatory,\n",
    "            - self.letter_to_feature_inhibitory\n",
    "        )\n",
    "        # For all positions\n",
    "        self.feature_to_letter_weights = block_diag(\n",
    "            *[feature_to_letter_weights_1 for _ in range(4)])\n",
    "        \n",
    "    def initialize_nodes(self):\n",
    "        self.feature_nodes = np.ones((self.position_count, feature_count)) * self.r_feature\n",
    "        self.letter_nodes = np.ones((self.position_count, letter_count)) * self.r_letter\n",
    "        \n",
    "    def present_word(self, word: str):\n",
    "        \"\"\"Show a word to the model\"\"\"\n",
    "        features_present = np.array([features_binary[letter] for letter in word])\n",
    "        # Set features present in the word to the maximum activation\n",
    "        self.feature_nodes = self.M * features_present\n",
    "    \n",
    "    def calculate_decay(self):\n",
    "        feature_decay = (self.feature_nodes - self.r_feature) * self.theta\n",
    "        letter_decay = (self.letter_nodes - self.r_letter) * self.theta\n",
    "        return feature_decay, letter_decay\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_layer_to_layer_input(layer_A, layer_B, weights):\n",
    "        \"\"\"\n",
    "        Calculates net input from layer_A to layer_B with weights connecting them.\n",
    "        This function is necessary because we prefer to keep nodes in 4 x N arrays instead of long vectors\n",
    "        \"\"\"\n",
    "        # Only the active (activation > 0) nodes get to send signals.\n",
    "        # Inhibitory connections have negative weights in this implementation\n",
    "        return (layer_A.ravel() * (layer_A.ravel() > 0) @ weights).reshape(layer_B.shape)\n",
    "    \n",
    "    def calculate_neighbours_effect(self):\n",
    "        # There are no connections to the feature level except for the visual input\n",
    "        feature_neighbours_effect = np.zeros(self.feature_nodes.shape)\n",
    "\n",
    "        # Equation 1\n",
    "        # Only the active (activation > 0) nodes get to send signals.\n",
    "\n",
    "        net_input = (\n",
    "            self.calculate_layer_to_layer_input(\n",
    "                self.feature_nodes, \n",
    "                self.letter_nodes, \n",
    "                self.feature_to_letter_weights)\n",
    "            + self.calculate_layer_to_layer_input(\n",
    "                self.letter_nodes, \n",
    "                self.letter_nodes, \n",
    "                self.letter_to_letter_weights))\n",
    "\n",
    "        # Equation 2 and 3\n",
    "        letter_neighbours_effect = np.where(\n",
    "            net_input > 0,\n",
    "            net_input * (self.M - self.letter_nodes),\n",
    "            net_input * (self.letter_nodes - self.m)\n",
    "        )\n",
    "\n",
    "        return feature_neighbours_effect, letter_neighbours_effect\n",
    "    \n",
    "    def run_cycle(self):        \n",
    "        feature_decay, letter_decay = self.calculate_decay()\n",
    "        feature_neighbours_effect, letter_neighbours_effect = self.calculate_neighbours_effect()\n",
    "        \n",
    "        self.feature_nodes += - feature_decay + feature_neighbours_effect\n",
    "        self.letter_nodes += - letter_decay + letter_neighbours_effect\n",
    "        \n",
    "    def run_n_cycles(self, n: int):\n",
    "        for _ in range(n):\n",
    "            self.run_cycle()\n",
    "            \n",
    "    def print_active_letters(self):\n",
    "        for i in range(4):\n",
    "            active_letters = np.array(alphabet)[iam.letter_nodes[i] > 0]\n",
    "            print(f'letter {i+1}: {active_letters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter 1: ['W']\n",
      "letter 2: ['O' 'Q']\n",
      "letter 3: ['R']\n",
      "letter 4: ['K']\n"
     ]
    }
   ],
   "source": [
    "iam = IAM()\n",
    "iam.present_word('WORK')\n",
    "iam.run_cycle()\n",
    "iam.print_active_letters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter 1: ['W']\n",
      "letter 2: ['Q']\n",
      "letter 3: ['R']\n",
      "letter 4: ['K']\n"
     ]
    }
   ],
   "source": [
    "iam.initialize_nodes()\n",
    "iam.present_word('WQRK')\n",
    "iam.run_cycle()\n",
    "iam.print_active_letters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cogmod_1_IAM]",
   "language": "python",
   "name": "conda-env-cogmod_1_IAM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
